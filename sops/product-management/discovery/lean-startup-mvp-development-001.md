# Lean Startup MVP Development Process

## Metadata
- **Source**: Eric Ries - Lenny's Podcast
- **Domain**: Product Management / Discovery
- **Type**: Process SOP
- **Applicable To**: Founders, Product Managers, Startup Teams
- **Company Stage**: Pre-seed to Series A (especially early stage)
- **Difficulty**: Intermediate

## Overview
The Minimum Viable Product (MVP) is not about building low-quality products, but about finding the minimum scope needed to test your most critical assumptions. The MVP process involves identifying what you need to learn, determining the smallest product increment that can generate that learning, and running experiments before committing to full development.

## When to Use
- Starting a new product or feature from scratch
- You have uncertainty about customer needs or product-market fit
- You want to validate assumptions before full-scale development
- Resources (time/money) are constrained
- Market expectations are unclear for your product category
- You're tempted to build for months before getting customer feedback

## Prerequisites
- A clear vision of what problem you're trying to solve
- Willingness to admit what you don't know
- Access to potential customers for feedback
- Understanding that products take time to build regardless of approach
- Acceptance that some work will be wasted (this is expected)

## Core Principle: The Chain of Deductive Reasoning

```
[VISION]
    ↓
[What do we need to learn?]
    ↓
[What assumptions must be true?]
    ↓
[How can we test these assumptions?]
    ↓
[MINIMUM VIABLE PRODUCT]
```

## Procedure

### Step 1: Clarify What You Need to Learn

**The #1 problem with MVP development**: People aren't clear on what they want to learn because they don't want to admit they don't know.

**Ask yourself:**
- What do I assume to be true about customers?
- What must be true for this product to succeed?
- What am I most uncertain about?
- What would cause me to change direction?

**Document your learning goals explicitly:**
- "We need to learn if customers will pay for this"
- "We need to learn if the value proposition resonates"
- "We need to learn if customers understand how to use this"

**Anti-pattern**: Saying "We need to build features X, Y, Z" instead of "We need to learn if customers have problem A"

### Step 2: Understand You Don't Know M, V, or P

Ben Silbermann (Pinterest founder) noted: "The hard part about MVP is you don't know what Minimum is, you don't know what Viable is."

**Eric Ries adds**: You also don't know what Product is.

**Common unknowns:**
- **Minimum**: What's the bar for your specific market? (iOS apps ≠ B2B SaaS ≠ marketplace)
- **Viable**: What's enough to test your hypothesis vs. enough to ship?
- **Product**: Is this a service? A feature? A platform? A tool?

**Action**: Accept uncertainty as part of the process. You'll discover these through experimentation.

### Step 3: Leverage Natural Experiments During Development

**Key insight**: Products take time to build. At some point, you'll have half the product built. Use that moment.

**The logic:**
1. Building takes time (weeks, months, years)
2. At 10% complete, you have something
3. At 50% complete, you have half a product
4. **These partial states are natural experiments**

**Practical application:**
- When you hit 10%, 25%, 50% completion, show it to customers
- "It doesn't cost you anything extra. You had to build it anyway."
- Let customers tell you it's "only half as good" or "unusable"
- **This is excellent feedback, not a failure**

**Example**: If building takes 6 months, don't wait 6 months for first customer contact. Show them what you have at month 2, even if incomplete.

### Step 4: Run a Series of Experiments, Not Just One

**Critical mistake**: Drawing conclusions from a single experiment.

**Real example from transcript:**
- Team launched with 0% signups
- Reaction: "It's a catastrophe. We made the wrong value prop."
- **Eric's response**: "How do you know? From one experiment you can never know."

**Process:**
1. Design experiment 1 (test assumption A)
2. Run it, gather data
3. Analyze: What did we learn?
4. Design experiment 2 (test refined hypothesis or assumption B)
5. Run it, gather data
6. Compare results across experiments
7. Look for patterns before drawing conclusions

**Minimum**: 3-5 experiments before making major pivots

### Step 5: Determine Context-Specific "Minimum"

**"Minimum" is fluid and context-dependent.** The message isn't "build low quality." It's "build as little as possible for YOUR context."

**Factors affecting minimum bar:**

**Market expectations:**
- iOS app store: Higher bar (polish, performance)
- B2B enterprise: Different bar (security, integrations, support)
- Early adopter community: Lower bar (tolerance for rough edges)
- Crowded market: Higher bar (must differentiate)

**Your specific advantage:**
- If you have deep domain expertise: You can get away with less upfront validation
- If you're new to space: Need more experimentation
- If you have distribution: Can test with rougher product
- If you're unknown: Need more polish to get attention

**Example decision tree:**
```
Do you have proven taste in this domain? (shipped successful products)
├─ Yes: "I'll give you benefit of the doubt" - but still validate with 10 customers
└─ No: Definitely do extensive experimentation
```

### Step 6: Test with Small Customer Cohorts

**Before broad launch, test with small groups:**

**Recommended approach:**
- **10 customers** for initial validation ("have them throw up about how horrible it is")
- If they hate it: "At least confirms we're on the right track" (you learned something)
- If they love it: Double-check with another 10
- Look for patterns across multiple small cohorts

**Why small groups:**
- Faster to recruit
- Easier to get deep feedback
- Can iterate quickly
- Less commitment if you need to change direction

### Step 7: Embrace "Less Is More" Examples

**The startup literature is full of cases where the simpler, smaller thing won:**

**Examples:**
- Craigslist (simple text) vs. elaborate classified ad systems
- Google (simple search box) vs. portal competitors
- Twitter (140 characters) vs. robust blogging platforms
- WhatsApp (just messaging) vs. feature-rich social apps

**Lesson**: Sometimes constraints create value. The "incomplete" version might be the right version.

### Step 8: Accept That Work Will Be Wasted

**Fundamental truth**: Most of what you build won't work as expected.

**Eric's perspective**: "A lot of the work you're doing is going to go to waste anyway. Why spend all this time building it?"

**Mindset shift:**
- Wasted work is inevitable
- The question is: How much work gets wasted?
- **MVP minimizes waste** by learning before building everything
- Would you rather waste 2 weeks or 6 months?

**This doesn't mean:**
- Build carelessly
- Ship broken products
- Ignore quality

**It means:**
- Build the minimum to learn
- Expect to change direction
- Preserve resources for iteration

## Expected Outcomes
- Validated (or invalidated) assumptions before full development
- Faster learning cycles (weeks instead of months)
- Reduced waste (less code thrown away)
- Earlier customer contact and feedback
- More confidence in product direction
- Clear decision points about whether to pivot or persevere

## Common Pitfalls

### Not admitting what you don't know
- **Problem**: "We know customers want this"
- **Solution**: Force yourself to articulate uncertainties as hypotheses
- **Test**: Can you state what would prove you wrong?

### Confusing MVP with prototype
- **Prototype**: Demonstrates an idea (may be fake)
- **MVP**: Actually delivers value to customers (must be real)
- **Both are valid** but serve different purposes

### Setting "minimum" too high
- **Problem**: "We need all these features before customers can use it"
- **Solution**: Challenge each "must-have" - can we test without it?
- **Reality check**: Show incomplete version to customers, let them tell you what's missing

### Setting "minimum" too low for your context
- **Problem**: Launching something so incomplete it can't generate valid feedback
- **Solution**: Understand minimum bar for your specific market
- **Test**: Would customers in your category engage with this?

### Drawing conclusions from single experiments
- **Problem**: One test with 0% conversion → "The idea is wrong"
- **Solution**: Run 3-5 experiments minimum before major decisions
- **Remember**: You're testing the test as much as the product

### Waiting for "the right time" to get customer feedback
- **Problem**: "Let's finish the next sprint, then we'll show customers"
- **Solution**: Show customers NOW, at 10%, 25%, 50% complete
- **Benefit**: These are natural experiments you're getting for free

### Optimizing for current customers instead of testing assumptions
- **Problem**: Making MVP too polished to avoid embarrassment
- **Solution**: Early customers should be innovation partners, not judges
- **Mindset**: "Have them throw up about how horrible it is" = valuable data

## Related SOPs
- Build-Measure-Learn Loop Framework
- Pivot vs. Persevere Decision Framework
- Validated Learning Methods
- Customer Interview Techniques
- Assumption Testing Framework

## AI Integration Notes

### Context signals that should trigger this SOP:
- User mentions "starting new product," "first version," "launching soon"
- Questions about "how much to build," "when to ship," "feature scope"
- Uncertainty about customer needs or product-market fit
- Limited resources (time, money, team)
- Debates about "must-have" vs. "nice-to-have" features

### How to adapt for different situations:
- **B2C mobile app**: Higher bar on polish, lower bar on features
- **B2B SaaS**: Higher bar on security/compliance, can iterate on UX
- **Marketplace**: Need both sides working, but can be manual at first
- **Developer tools**: Can be rough if saving significant time
- **Consumer social**: Need to feel complete even if feature-light

### Key questions to ask users:
1. "What are you most uncertain about?"
2. "What do you need to learn?"
3. "What could you show customers right now?"
4. "What would prove you wrong?"
5. "How many experiments have you run?"

### Limitations and edge cases:
- Some products genuinely have high minimum bars (medical devices, financial systems)
- Regulatory requirements may prevent early testing
- Network effects products need critical mass (but can often fake it initially)
- Some markets punish early mediocrity (hard to recover reputation)

### The meta-lesson:
MVP is not a quality standard, it's a learning strategy. The goal is to minimize the cost of being wrong about your assumptions.
